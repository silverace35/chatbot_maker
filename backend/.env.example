# Server
PORT=4000

# Store mode: 'memory' or 'postgres'
STORE_MODE=postgres

DB_HOST=localhost
DB_PORT=15432
DB_NAME=electron_chat
DB_USER=electron_user
DB_PASSWORD=electron_password

# other models: llama2:7b, llama2:13b, llama3:8b, llama3:70b, gpt4all-j-v1.3-groovy
OLLAMA_DEFAULT_MODEL=llama3.1:8b
OLLAMA_TIMEOUT_MS=120000

OLLAMA_ENABLED=true
OLLAMA_URL=http://localhost:11434

# Warmup: précharger le modèle LLM au démarrage du serveur pour éviter le cold start
# Le modèle sera chargé en mémoire GPU et y restera (keep_alive=-1)
# Mettre à 'false' pour désactiver le warmup automatique
OLLAMA_WARMUP=true

# IMPORTANT: Configuration Ollama pour le parallélisme
# Ollama ne peut pas interrompre une génération en cours.
# Pour permettre de nouvelles requêtes pendant qu'une génération est en cours,
# configurez ces variables dans l'environnement Ollama (pas ici, dans le service Ollama) :
#   OLLAMA_NUM_PARALLEL=2  (permet 2 requêtes simultanées)
#   OLLAMA_MAX_LOADED_MODELS=2  (garde 2 modèles en mémoire)
#
# Sous Docker, ajoutez dans docker-compose.yml:
#   environment:
#    - OLLAMA_NUM_PARALLEL=2

# Fenêtre d'historique envoyée au LLM (pour limiter la taille du prompt)
LLM_MAX_HISTORY=16

EMBEDDING_PROVIDER=ollama
# Options: nomic-embed-text (768d), mxbai-embed-large (1024d), all-minilm (384d)
EMBEDDING_MODEL=nomic-embed-text
