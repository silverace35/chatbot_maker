# Server
PORT=4000

# Store mode: 'memory' or 'postgres'
STORE_MODE=postgres

DB_HOST=localhost
DB_PORT=15432
DB_NAME=electron_chat
DB_USER=electron_user
DB_PASSWORD=electron_password

# other models: llama2:7b, llama2:13b, llama3:8b, llama3:70b, gpt4all-j-v1.3-groovy
OLLAMA_DEFAULT_MODEL=llama3.1:8b
OLLAMA_TIMEOUT_MS=60000

OLLAMA_ENABLED=true
OLLAMA_URL=http://localhost:11434

# Warmup: précharger le modèle LLM au démarrage du serveur pour éviter le cold start
# Le modèle sera chargé en mémoire GPU et y restera (keep_alive=-1)
# Mettre à 'false' pour désactiver le warmup automatique
OLLAMA_WARMUP=true

# Fenêtre d'historique envoyée au LLM (pour limiter la taille du prompt)
LLM_MAX_HISTORY=16

EMBEDDING_PROVIDER=ollama
# Options: nomic-embed-text (768d), mxbai-embed-large (1024d), all-minilm (384d)
EMBEDDING_MODEL=nomic-embed-text
