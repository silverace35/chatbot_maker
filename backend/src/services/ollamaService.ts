import { Message } from '../models/chatSession';

export interface OllamaGenerateRequest {
  model: string;
  prompt: string;
  system?: string;
  stream?: boolean;
  options?: {
    temperature?: number;
    top_p?: number;
    top_k?: number;
  };
}

export interface OllamaChatRequest {
  model: string;
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  stream?: boolean;
  keep_alive?: number | string; // -1 pour garder en m√©moire ind√©finiment, ou dur√©e comme "5m"
  options?: {
    temperature?: number;
    top_p?: number;
    top_k?: number;
  };
}

export interface OllamaModel {
  name: string;
  modified_at?: string;
  size?: number;
}

export interface OllamaModelsResponse {
  models: OllamaModel[];
}

export interface OllamaResponse {
  model: string;
  created_at: string;
  message?: {
    role: string;
    content: string;
  };
  response?: string;
  done: boolean;
}

export interface OllamaStreamChunk {
  content: string;
  done: boolean;
}

export interface OllamaEmbeddingRequest {
  model: string;
  prompt: string;
}

export interface OllamaEmbeddingResponse {
  embedding: number[];
}

/**
 * Service d'int√©gration avec Ollama
 * 
 * Ce service communique avec l'API Ollama pour g√©n√©rer des r√©ponses LLM r√©elles.
 * Il supporte plusieurs mod√®les et g√®re les erreurs de connexion.
 */
export class OllamaService {
  private baseUrl: string;
  private defaultModel: string;
  private timeout: number;
  private isWarmedUp: boolean = false;
  private warmupInProgress: boolean = false;

  constructor(
    baseUrl: string = process.env.OLLAMA_URL || 'http://localhost:11434',
    defaultModel: string = process.env.OLLAMA_DEFAULT_MODEL || 'llama3.1:8b',
    timeout: number = parseInt(process.env.OLLAMA_TIMEOUT_MS || '600000', 10),
  ) {
    this.baseUrl = baseUrl;
    this.defaultModel = defaultModel;
    this.timeout = timeout;

    console.log('[OllamaService] Base URL =', this.baseUrl);
    console.log('[OllamaService] Default model =', this.defaultModel);
    console.log('[OllamaService] Timeout (ms) =', this.timeout);
  }

  /**
   * Pr√©charge le mod√®le en m√©moire GPU pour √©viter le cold start
   * Envoie une requ√™te minimale pour forcer Ollama √† charger le mod√®le
   */
  async warmup(model?: string): Promise<boolean> {
    if (this.isWarmedUp || this.warmupInProgress) {
      return this.isWarmedUp;
    }

    this.warmupInProgress = true;
    const selectedModel = model || this.defaultModel;

    console.log(`[OllamaService] üî• Warming up model "${selectedModel}"...`);
    const startTime = Date.now();

    try {
      // V√©rifier d'abord que le mod√®le est disponible
      const models = await this.listModels();
      if (!models.includes(selectedModel)) {
        console.warn(`[OllamaService] ‚ö†Ô∏è Model "${selectedModel}" not found. Available: ${models.join(', ')}`);
        this.warmupInProgress = false;
        return false;
      }

      // Envoyer une requ√™te minimale pour forcer le chargement du mod√®le
      // keep_alive: -1 maintient le mod√®le en m√©moire ind√©finiment
      const response = await fetch(`${this.baseUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: selectedModel,
          prompt: 'Hi',
          stream: false,
          options: {
            num_predict: 1, // G√©n√©rer seulement 1 token pour √™tre rapide
          },
          keep_alive: -1, // Garder le mod√®le charg√© ind√©finiment
        }),
      });

      if (response.ok) {
        const duration = Date.now() - startTime;
        console.log(`[OllamaService] ‚úÖ Model "${selectedModel}" warmed up in ${duration}ms`);
        this.isWarmedUp = true;
        this.warmupInProgress = false;
        return true;
      } else {
        console.warn(`[OllamaService] ‚ö†Ô∏è Warmup failed: ${response.statusText}`);
        this.warmupInProgress = false;
        return false;
      }
    } catch (error) {
      console.error('[OllamaService] ‚ùå Warmup error:', error instanceof Error ? error.message : error);
      this.warmupInProgress = false;
      return false;
    }
  }

  /**
   * V√©rifie si le mod√®le est pr√©charg√©
   */
  isModelWarmedUp(): boolean {
    return this.isWarmedUp;
  }

  /**
   * V√©rifie si Ollama est disponible
   */
  async isAvailable(): Promise<boolean> {
    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 5000); // 5 secondes timeout

      const response = await fetch(`${this.baseUrl}/api/tags`, {
        signal: controller.signal,
      });

      clearTimeout(timeoutId);
      return response.ok;
    } catch (error) {
      console.warn('Ollama not available:', error instanceof Error ? error.message : 'Unknown error');
      return false;
    }
  }

  /**
   * Liste les mod√®les disponibles dans Ollama
   */
  async listModels(): Promise<string[]> {
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`);
      if (!response.ok) {
        throw new Error(`Failed to list models: ${response.statusText}`);
      }

      const data = await response.json() as OllamaModelsResponse;
      return data.models?.map((m) => m.name) || [];
    } catch (error) {
      console.error('Error listing Ollama models:', error);
      return [];
    }
  }

  private async ensureModelAvailable(model: string): Promise<void> {
    const models = await this.listModels();
    if (!models.includes(model)) {
      const hint = models.length
        ? `Mod√®les disponibles: ${models.join(', ')}`
        : 'Aucun mod√®le n‚Äôest actuellement disponible dans Ollama. Pense √† lancer "ollama pull" ou √† t√©l√©charger le mod√®le via l‚Äôinterface Ollama.';
      throw new Error(
        `Le mod√®le Ollama "${model}" n'est pas disponible. ${hint}`,
      );
    }
  }

  /**
   * G√©n√®re une r√©ponse avec Ollama en utilisant l'API chat
   * 
   * @param messages Historique des messages (system, user, assistant)
   * @param model Mod√®le √† utiliser (optionnel, utilise le mod√®le par d√©faut sinon)
   * @param temperature Temp√©rature pour la g√©n√©ration (0-1)
   * @returns Contenu de la r√©ponse g√©n√©r√©e
   */
  async chat(
    messages: Message[],
    model?: string,
    temperature: number = 0.7,
  ): Promise<string> {
    const selectedModel = model || this.defaultModel;
    await this.ensureModelAvailable(selectedModel);

    // Convertir les messages au format Ollama
    const ollamaMessages = messages.map(msg => ({
      role: msg.role as 'system' | 'user' | 'assistant',
      content: msg.content,
    }));

    const request: OllamaChatRequest = {
      model: selectedModel,
      messages: ollamaMessages,
      stream: false,
      keep_alive: -1, // Garder le mod√®le charg√© en m√©moire GPU ind√©finiment
      options: {
        temperature,
      },
    };

    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), this.timeout);

      const response = await fetch(`${this.baseUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.statusText}`);
      }

      const data = await response.json() as OllamaResponse;
      
      if (!data.message?.content) {
        throw new Error('No content in Ollama response');
      }

      return data.message.content;
    } catch (error) {
      if (error instanceof Error) {
        if (error.name === 'AbortError') {
          throw new Error('Ollama request timeout');
        }
        throw new Error(`Ollama chat error: ${error.message}`);
      }
      throw new Error('Unknown Ollama error');
    }
  }

  /**
   * G√©n√®re une r√©ponse simple avec un prompt (API generate)
   * Utilis√© principalement pour des tests simples
   */
  async generate(
    prompt: string,
    systemContext?: string,
    model?: string,
    temperature: number = 0.7,
  ): Promise<string> {
    const selectedModel = model || this.defaultModel;
    await this.ensureModelAvailable(selectedModel);

    const request: OllamaGenerateRequest = {
      model: selectedModel,
      prompt,
      system: systemContext,
      stream: false,
      options: {
        temperature,
      },
    };

    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), this.timeout);

      const response = await fetch(`${this.baseUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.statusText}`);
      }

      const data = await response.json() as OllamaResponse;
      
      if (!data.response) {
        throw new Error('No response in Ollama generate response');
      }

      return data.response;
    } catch (error) {
      if (error instanceof Error) {
        if (error.name === 'AbortError') {
          throw new Error('Ollama request timeout');
        }
        throw new Error(`Ollama generate error: ${error.message}`);
      }
      throw new Error('Unknown Ollama error');
    }
  }

  /**
   * T√©l√©charge un mod√®le depuis Ollama Hub
   * Note: Cette op√©ration peut prendre plusieurs minutes
   */
  async pullModel(modelName: string): Promise<void> {
    try {
      const response = await fetch(`${this.baseUrl}/api/pull`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ name: modelName }),
      });

      if (!response.ok) {
        throw new Error(`Failed to pull model: ${response.statusText}`);
      }

      // Note: Le streaming de progression n'est pas g√©r√© ici
      // En production, on pourrait streamer le statut du t√©l√©chargement
      console.log(`Model ${modelName} pull initiated`);
    } catch (error) {
      console.error('Error pulling Ollama model:', error);
      throw error;
    }
  }

  /**
   * G√©n√®re un embedding pour un texte donn√©
   * 
   * @param text Texte √† transformer en embedding
   * @param model Mod√®le d'embedding √† utiliser (par d√©faut: nomic-embed-text)
   * @returns Le vecteur d'embedding
   */
  async generateEmbedding(
    text: string,
    model: string = 'nomic-embed-text'
  ): Promise<number[]> {
    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 30000); // 30 secondes timeout

      const request: OllamaEmbeddingRequest = {
        model,
        prompt: text,
      };

      const response = await fetch(`${this.baseUrl}/api/embeddings`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        throw new Error(`Ollama embeddings API error: ${response.statusText}`);
      }

      const data = await response.json() as OllamaEmbeddingResponse;

      if (!data.embedding || !Array.isArray(data.embedding)) {
        throw new Error('Invalid embedding response from Ollama');
      }

      return data.embedding;
    } catch (error) {
      if (error instanceof Error) {
        if (error.name === 'AbortError') {
          throw new Error('Ollama embedding request timeout');
        }
        throw new Error(`Ollama embedding error: ${error.message}`);
      }
      throw new Error('Unknown Ollama embedding error');
    }
  }

  /**
   * G√©n√®re des embeddings pour plusieurs textes (batch)
   * 
   * @param texts Tableau de textes √† transformer en embeddings
   * @param model Mod√®le d'embedding √† utiliser (par d√©faut: nomic-embed-text)
   * @returns Tableau de vecteurs d'embeddings
   */
  async generateEmbeddings(
    texts: string[],
    model: string = 'nomic-embed-text'
  ): Promise<number[][]> {
    // Ollama doesn't support batch embedding natively, so we process sequentially
    // In production, consider implementing parallel processing with rate limiting
    const embeddings: number[][] = [];
    
    for (const text of texts) {
      const embedding = await this.generateEmbedding(text, model);
      embeddings.push(embedding);
    }

    return embeddings;
  }

  /**
   * G√©n√®re une r√©ponse avec Ollama en utilisant l'API chat en mode streaming
   *
   * @param messages Historique des messages (system, user, assistant)
   * @param options Options de streaming (mod√®le, temp√©rature, signal d'annulation)
   * @returns Un it√©rateur asynchrone de chunks de texte
   */
  async *chatStream(
    messages: Message[],
    options?: { model?: string; temperature?: number; signal?: AbortSignal },
  ): AsyncGenerator<OllamaStreamChunk, void, unknown> {
    const selectedModel = options?.model || this.defaultModel;
    await this.ensureModelAvailable(selectedModel);

    const ollamaMessages = messages.map(msg => ({
      role: msg.role as 'system' | 'user' | 'assistant',
      content: msg.content,
    }));

    const request: OllamaChatRequest = {
      model: selectedModel,
      messages: ollamaMessages,
      stream: true,
      keep_alive: -1, // Garder le mod√®le charg√© en m√©moire GPU ind√©finiment
      options: {
        temperature: options?.temperature ?? 0.7,
      },
    };

    const controller = new AbortController();
    const externalSignal = options?.signal;

    const onAbort = () => {
      if (!controller.signal.aborted) {
        controller.abort();
      }
    };

    if (externalSignal) {
      if (externalSignal.aborted) {
        controller.abort();
      } else if (typeof externalSignal.addEventListener === 'function') {
        externalSignal.addEventListener('abort', onAbort, { once: true });
      }
    }

    const timeoutId = setTimeout(() => controller.abort(), this.timeout);

    try {
      const response = await fetch(`${this.baseUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(request),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok || !response.body) {
        throw new Error(`Ollama API error (stream): ${response.statusText}`);
      }

      const reader = response.body.getReader();
      const decoder = new TextDecoder();

      try {
        while (true) {
          // V√©rifier l'annulation AVANT de lire
          if (externalSignal?.aborted || controller.signal.aborted) {
            console.log('[OllamaService] Stream aborted by client, cancelling reader...');
            await reader.cancel('Client aborted');
            return;
          }

          const { value, done } = await reader.read();
          if (done) {
            break;
          }

          // V√©rifier l'annulation APR√àS la lecture aussi
          if (externalSignal?.aborted || controller.signal.aborted) {
            console.log('[OllamaService] Stream aborted by client after read, cancelling...');
            await reader.cancel('Client aborted');
            return;
          }

          const text = decoder.decode(value, { stream: true });
          const lines = text.split(/\n/).filter(Boolean);

          for (const line of lines) {
            try {
              const data = JSON.parse(line) as OllamaResponse;
              const contentPart = data.message?.content || data.response || '';
              const doneFlag = data.done;

              if (contentPart) {
                yield { content: contentPart, done: !!doneFlag };
              }

              if (doneFlag) {
                return;
              }
            } catch {
              // Ligne invalide, on l'ignore simplement
              continue;
            }
          }
        }
      } finally {
        // S'assurer que le reader est annul√© si le signal a √©t√© abort√©
        if (externalSignal?.aborted || controller.signal.aborted) {
          try {
            await reader.cancel('Cleanup after abort');
          } catch {
            // Ignorer les erreurs de cancel
          }
        }
        reader.releaseLock();
      }
    } catch (error) {
      if ((error as any).name === 'AbortError' || externalSignal?.aborted) {
        console.log('[OllamaService] Stream aborted or timed out');
        // Annulation ou timeout : on arr√™te silencieusement le stream
        return;
      }
      throw error;
    } finally {
      clearTimeout(timeoutId);
      if (externalSignal && typeof externalSignal.removeEventListener === 'function') {
        externalSignal.removeEventListener('abort', onAbort as any);
      }
    }
  }
}

export const ollamaService = new OllamaService();
