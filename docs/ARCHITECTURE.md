# Architecture Macro - ChatBot Maker

Ce document pr√©sente l'architecture globale de l'application ChatBot Maker, une solution desktop permettant de cr√©er et interagir avec des chatbots personnalis√©s aliment√©s par un LLM local.

---

## Diagramme d'Architecture Globale

```mermaid
flowchart TB
    subgraph Desktop["üñ•Ô∏è Application Desktop"]
        subgraph Electron["Electron"]
            Main["Main Process<br/>(electron/main.js)"]
            Preload["Preload Script<br/>(electron/preload.js)"]
        end
        
        subgraph Frontend["Frontend React"]
            Renderer["Renderer Process<br/>(renderer/)"]
            UI["Material-UI<br/>Components"]
            Modules["Modules<br/>(chat, profile)"]
        end
        
        Main <--> Preload
        Preload <--> Renderer
        Renderer --> UI
        Renderer --> Modules
    end
    
    subgraph Backend["üîß Backend Node.js"]
        Express["Express Server<br/>:4000"]
        Routes["Routes API<br/>(/api/chat, /api/profile)"]
        Services["Services<br/>(LLM, RAG, Embedding)"]
        Store["Store Layer<br/>(Memory/PostgreSQL)"]
        
        Express --> Routes
        Routes --> Services
        Services --> Store
    end
    
    subgraph Docker["üê≥ Services Docker"]
        subgraph DB["Base de Donn√©es"]
            PostgreSQL["PostgreSQL 16<br/>:15432"]
        end
        
        subgraph AI["Services IA"]
            Ollama["Ollama<br/>:11434"]
            Models["Mod√®les<br/>llama3.1:8b<br/>nomic-embed-text"]
        end
        
        subgraph Vector["Vector Store"]
            Qdrant["Qdrant<br/>:6333/:6334"]
        end
        
        Ollama --> Models
    end
    
    subgraph Hardware["‚ö° Hardware"]
        GPU["GPU NVIDIA<br/>(RTX 4060+)"]
        CPU["CPU"]
    end
    
    %% Connexions principales
    Preload -->|"HTTP/REST<br/>Streaming"| Express
    Store -->|"SQL"| PostgreSQL
    Services -->|"Embeddings<br/>LLM Inference"| Ollama
    Services -->|"Vector Search"| Qdrant
    Ollama -->|"CUDA"| GPU
    Ollama -.->|"Fallback"| CPU
    
    %% Styles
    classDef electron fill:#47848F,color:#fff
    classDef react fill:#61DAFB,color:#000
    classDef node fill:#339933,color:#fff
    classDef docker fill:#2496ED,color:#fff
    classDef db fill:#336791,color:#fff
    classDef ai fill:#FF6F00,color:#fff
    classDef vector fill:#DC382D,color:#fff
    classDef hw fill:#76B900,color:#fff
    
    class Main,Preload electron
    class Renderer,UI,Modules react
    class Express,Routes,Services,Store node
    class PostgreSQL db
    class Ollama,Models ai
    class Qdrant vector
    class GPU,CPU hw
```

---

## Vue D√©taill√©e des Flux de Donn√©es

```mermaid
sequenceDiagram
    participant U as üë§ Utilisateur
    participant E as üñ•Ô∏è Electron
    participant R as ‚öõÔ∏è React Frontend
    participant B as üîß Backend Express
    participant P as üêò PostgreSQL
    participant O as ü¶ô Ollama
    participant Q as üî¥ Qdrant
    
    Note over U,Q: Flux de Chat Standard
    U->>R: Saisie message
    R->>E: window.api.chat.send()
    E->>B: POST /api/chat (stream)
    B->>P: R√©cup√®re session & profil
    B->>O: Generate (stream)
    O-->>B: Tokens (streaming)
    B-->>E: SSE Events
    E-->>R: onChunk callbacks
    R-->>U: Affichage progressif
    B->>P: Sauvegarde message
    
    Note over U,Q: Flux RAG (si activ√©)
    U->>R: Saisie message
    R->>E: window.api.chat.send()
    E->>B: POST /api/chat
    B->>O: Embedding query
    O-->>B: Vector [768d]
    B->>Q: Recherche similarit√©
    Q-->>B: Chunks pertinents
    B->>B: Augmente prompt
    B->>O: Generate + contexte
    O-->>B: R√©ponse enrichie
```

---

## 1. Electron (Application Desktop)

### Description
Electron encapsule l'application web dans une fen√™tre desktop native, permettant un d√©ploiement sur Windows, macOS et Linux sans navigateur externe.

### Composants

| Fichier | R√¥le |
|---------|------|
| `electron/main.js` | Processus principal : cr√©ation de fen√™tre, gestion du cycle de vie |
| `electron/preload.js` | Bridge s√©curis√© entre le renderer et les APIs Node.js via `contextBridge` |

### Configuration de s√©curit√©
```javascript
// Param√®tres de s√©curit√© dans main.js
webPreferences: {
    nodeIntegration: false,      // D√©sactiv√© pour s√©curit√©
    contextIsolation: true,      // Isolation du contexte
    preload: 'preload.js'        // Script de bridge
}
```

### API expos√©e au Frontend
Le `preload.js` expose une API s√©curis√©e √† `window.api` :

```typescript
window.api = {
    chat: { send, getSession, getSessions, deleteSession },
    profile: { getAll, getById, create, update, delete },
    rag: { uploadResources, getResources, startIndexing, getIndexingStatus }
}
```

---

## 2. Frontend React (Renderer)

### Description
Interface utilisateur moderne construite avec React, TypeScript et Material-UI. Communique avec le backend via les APIs expos√©es par Electron.

### Structure des modules

```
renderer/src/
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ chat/           # Page de chat, liste messages, input
‚îÇ   ‚îî‚îÄ‚îÄ profile/        # Gestion des profils de chatbot
‚îú‚îÄ‚îÄ services/           # Appels API (chat, profile, rag)
‚îú‚îÄ‚îÄ components/         # Composants r√©utilisables
‚îú‚îÄ‚îÄ contexts/           # Contextes React (th√®me, auth)
‚îî‚îÄ‚îÄ theme/              # Configuration Material-UI
```

### Technologies

| Technologie | Version | Usage |
|-------------|---------|-------|
| React | 18.x | Framework UI |
| TypeScript | 5.x | Typage statique |
| Vite | 5.x | Bundler & dev server |
| Material-UI | 5.x | Composants UI |
| React Router | 6.x | Navigation |

### Variables d'environnement Frontend

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `VITE_API_URL` | http://localhost:4000 | URL du backend (dev) |

---

## 3. Backend Express (API Server)

### Description
Serveur API REST en Node.js/TypeScript qui orchestre toutes les op√©rations : gestion des profils, sessions de chat, appels LLM et RAG.

### Architecture en couches

```mermaid
flowchart LR
    A[Routes] --> B[Services]
    B --> C[Store]
    C --> D[(PostgreSQL)]
    
    B --> E[Ollama]
    B --> F[Qdrant]
```

| Couche | Dossier | Responsabilit√© |
|--------|---------|----------------|
| **Routes** | `src/routes/` | Points d'entr√©e HTTP, validation |
| **Services** | `src/services/` | Logique m√©tier, orchestration |
| **Store** | `src/store/` | Abstraction persistance (memory/postgres) |
| **Models** | `src/models/` | D√©finitions TypeScript des entit√©s |

### Routes principales

| Endpoint | M√©thode | Description |
|----------|---------|-------------|
| `/api/chat` | POST | Envoi de message (streaming) |
| `/api/chat/sessions` | GET | Liste des sessions |
| `/api/profile` | CRUD | Gestion des profils |
| `/api/profile/:id/resources` | POST | Upload fichiers RAG |
| `/api/profile/:id/index` | POST | Lancement indexation |
| `/api/indexing-jobs/:id` | GET | Statut d'indexation |
| `/health` | GET | Health check |

### Variables d'environnement Backend

Fichier : `backend/.env`

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `PORT` | 4000 | Port du serveur Express |
| `STORE_MODE` | postgres | Mode de stockage : `memory` ou `postgres` |
| `DB_HOST` | localhost | H√¥te PostgreSQL |
| `DB_PORT` | 15432 | Port PostgreSQL |
| `DB_NAME` | electron_chat | Nom de la base de donn√©es |
| `DB_USER` | electron_user | Utilisateur PostgreSQL |
| `DB_PASSWORD` | electron_password | Mot de passe PostgreSQL |
| `OLLAMA_ENABLED` | true | Activer/d√©sactiver Ollama |
| `OLLAMA_URL` | http://localhost:11434 | URL du service Ollama |
| `OLLAMA_DEFAULT_MODEL` | llama3.1:8b | Mod√®le LLM par d√©faut |
| `OLLAMA_TIMEOUT_MS` | 120000 | Timeout des requ√™tes (ms) |
| `OLLAMA_WARMUP` | true | Pr√©charger le mod√®le au d√©marrage |
| `LLM_MAX_HISTORY` | 16 | Messages historiques envoy√©s au LLM |
| `EMBEDDING_PROVIDER` | ollama | Provider d'embeddings |
| `EMBEDDING_MODEL` | nomic-embed-text | Mod√®le d'embeddings |

---

## 4. PostgreSQL (Base de Donn√©es)

### Description
Base de donn√©es relationnelle stockant les profils, sessions de chat, messages et m√©tadonn√©es des ressources RAG.

### Sch√©ma simplifi√©

```mermaid
erDiagram
    profiles ||--o{ chat_sessions : "poss√®de"
    profiles ||--o{ resources : "contient"
    profiles ||--o{ indexing_jobs : "d√©clenche"
    chat_sessions ||--o{ messages : "contient"
    resources ||--o{ resource_chunks : "d√©coup√© en"
    resource_chunks ||--o{ resource_embeddings : "vectoris√© par"
    
    profiles {
        varchar id PK
        varchar name
        text system_context
        boolean rag_enabled
        varchar index_status
    }
    
    chat_sessions {
        varchar id PK
        varchar profile_id FK
        timestamp created_at
    }
    
    messages {
        serial id PK
        varchar session_id FK
        varchar role
        text content
    }
    
    resources {
        varchar id PK
        varchar profile_id FK
        varchar type
        varchar original_name
        boolean indexed
    }
```

> üìÑ Documentation compl√®te : [`docs/DATABASE_MPD.md`](DATABASE_MPD.md)

### Configuration Docker

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `POSTGRES_DB` | electron_chat | Nom de la base |
| `POSTGRES_USER` | electron_user | Utilisateur |
| `POSTGRES_PASSWORD` | electron_password | Mot de passe |
| `POSTGRES_PORT` | 15432 | Port expos√© |

### Connexion

```bash
# Depuis le host
psql -h localhost -p 15432 -U electron_user -d electron_chat

# Depuis Docker
docker exec -it electron-chat-postgres psql -U electron_user -d electron_chat
```

---

## 5. Ollama (LLM Local)

### Description
Serveur d'inf√©rence LLM local permettant d'ex√©cuter des mod√®les de langage sur GPU NVIDIA ou CPU. G√®re √† la fois la g√©n√©ration de texte et les embeddings.

### Mod√®les utilis√©s

| Mod√®le | Taille | Usage | Dimensions |
|--------|--------|-------|------------|
| `llama3.1:8b` | ~4.7GB | G√©n√©ration de r√©ponses | - |
| `nomic-embed-text` | ~274MB | Embeddings RAG | 768 |

### Architecture de d√©ploiement

```mermaid
flowchart LR
    subgraph Docker
        Ollama["Ollama Server<br/>:11434"]
        Init["ollama-init<br/>(t√©l√©chargement mod√®les)"]
    end
    
    subgraph GPU
        VRAM["VRAM GPU<br/>8GB+"]
    end
    
    Init -->|"ollama pull"| Ollama
    Ollama -->|"CUDA"| VRAM
```

### Configuration Docker

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `OLLAMA_PORT` | 11434 | Port HTTP API |
| `OLLAMA_NUM_PARALLEL` | 1 | Requ√™tes simultan√©es |
| `OLLAMA_MAX_LOADED_MODELS` | 1 | Mod√®les en VRAM |
| `OLLAMA_DEFAULT_MODEL` | llama3.1:8b | Mod√®le par d√©faut |

### API Ollama utilis√©e

| Endpoint | Usage |
|----------|-------|
| `POST /api/generate` | G√©n√©ration de texte (streaming) |
| `POST /api/embeddings` | G√©n√©ration d'embeddings |
| `GET /api/tags` | Liste des mod√®les |
| `GET /api/ps` | Mod√®les charg√©s en m√©moire |

### Support GPU

```yaml
# docker-compose.yml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

**Pr√©requis GPU :**
- Driver NVIDIA 525+
- Docker Desktop avec GPU support
- GPU avec 8GB+ VRAM recommand√©

### Commandes utiles

```bash
# V√©rifier les mod√®les install√©s
docker exec electron-chat-ollama ollama list

# Tester le LLM
curl http://localhost:11434/api/generate -d '{"model":"llama3.1:8b","prompt":"Hello"}'

# Voir les mod√®les charg√©s
curl http://localhost:11434/api/ps
```

---

## 6. Qdrant (Vector Store)

### Description
Base de donn√©es vectorielle haute performance pour le stockage et la recherche des embeddings RAG. Permet la recherche par similarit√© s√©mantique.

### Architecture RAG

```mermaid
flowchart LR
    subgraph Indexation
        Doc["üìÑ Document"] --> Chunk["Chunks<br/>(~500 tokens)"]
        Chunk --> Embed["Embedding<br/>(768d)"]
        Embed --> Store["Qdrant<br/>Collection"]
    end
    
    subgraph Recherche
        Query["‚ùì Question"] --> QEmbed["Embedding<br/>Question"]
        QEmbed --> Search["Recherche<br/>Similarit√©"]
        Search --> Results["Top K<br/>R√©sultats"]
    end
    
    Store --> Search
```

### Configuration Docker

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `QDRANT_PORT` | 6333 | Port HTTP REST |
| `QDRANT_GRPC_PORT` | 6334 | Port gRPC |

### Structure des collections

Chaque profil avec RAG activ√© a sa propre collection :

```
Collection: profile_{profileId}_{embeddingModel}
‚îú‚îÄ‚îÄ Vectors: float[768]
‚îú‚îÄ‚îÄ Payload:
‚îÇ   ‚îú‚îÄ‚îÄ chunk_id
‚îÇ   ‚îú‚îÄ‚îÄ resource_id
‚îÇ   ‚îú‚îÄ‚îÄ resource_name
‚îÇ   ‚îú‚îÄ‚îÄ content
‚îÇ   ‚îî‚îÄ‚îÄ metadata
```

### Param√®tres de recherche

| Param√®tre | D√©faut | Description |
|-----------|--------|-------------|
| `topK` | 5 | Nombre de r√©sultats retourn√©s |
| `similarityThreshold` | 0.7 | Seuil de similarit√© minimum |

### API REST

```bash
# Lister les collections
curl http://localhost:6333/collections

# Info sur une collection
curl http://localhost:6333/collections/{collection_name}

# Dashboard web
open http://localhost:6333/dashboard
```

---

## 7. Docker Compose (Orchestration)

### Description
Orchestre tous les services conteneuris√©s avec leurs d√©pendances et volumes persistants.

### Services

```yaml
services:
  postgres:      # Base de donn√©es
  ollama:        # LLM Server (GPU)
  ollama-init:   # T√©l√©chargement mod√®les
  qdrant:        # Vector Store
```

### Volumes persistants

| Volume | Contenu |
|--------|---------|
| `postgres_data` | Donn√©es PostgreSQL |
| `ollama_data` | Mod√®les LLM t√©l√©charg√©s |
| `qdrant_data` | Collections vectorielles |

### D√©pendances

```mermaid
flowchart TD
    postgres["PostgreSQL"]
    ollama["Ollama"]
    init["ollama-init"]
    qdrant["Qdrant"]
    
    init -->|"depends_on: healthy"| ollama
```

### Commandes

```bash
# D√©marrer tous les services
docker-compose up -d

# Voir les logs
docker-compose logs -f

# Arr√™ter
docker-compose down

# Reset complet (supprime les donn√©es)
docker-compose down -v
```

---

## 8. Flux de Donn√©es Complet

### Chat Simple (sans RAG)

```
1. Utilisateur tape message
2. Frontend ‚Üí Preload ‚Üí Backend (POST /api/chat)
3. Backend r√©cup√®re profil + historique (PostgreSQL)
4. Backend construit prompt avec system_context
5. Backend appelle Ollama (streaming)
6. Ollama g√©n√®re tokens ‚Üí Backend ‚Üí SSE ‚Üí Frontend
7. Frontend affiche progressivement
8. Backend sauvegarde message (PostgreSQL)
```

### Chat avec RAG

```
1. Utilisateur tape message
2. Backend g√©n√®re embedding de la question (Ollama)
3. Backend recherche chunks similaires (Qdrant)
4. Backend augmente le prompt avec le contexte trouv√©
5. Backend appelle Ollama avec prompt enrichi
6. R√©ponse bas√©e sur les documents index√©s
```

### Indexation RAG

```
1. Utilisateur upload fichiers
2. Backend stocke fichiers + m√©tadonn√©es (PostgreSQL)
3. Utilisateur lance indexation
4. Backend d√©coupe en chunks (~500 tokens)
5. Backend g√©n√®re embeddings (Ollama)
6. Backend stocke vecteurs (Qdrant)
7. Backend met √† jour index_status = 'ready'
```

---

## 9. R√©sum√© des Ports

| Service | Port | Protocol | Usage |
|---------|------|----------|-------|
| Backend Express | 4000 | HTTP | API REST + SSE |
| Frontend Vite | 5173 | HTTP | Dev server |
| PostgreSQL | 15432 | TCP | SQL |
| Ollama | 11434 | HTTP | LLM API |
| Qdrant HTTP | 6333 | HTTP | Vector REST |
| Qdrant gRPC | 6334 | gRPC | Vector gRPC |

---

## 10. Fichiers de Configuration

| Fichier | Description |
|---------|-------------|
| `.env` (racine) | Variables Docker Compose |
| `backend/.env` | Configuration backend |
| `docker-compose.yml` | Orchestration Docker |
| `backend/database/init.sql` | Sch√©ma SQL initial |
| `renderer/vite.config.js` | Configuration Vite |
| `electron/main.js` | Configuration Electron |

